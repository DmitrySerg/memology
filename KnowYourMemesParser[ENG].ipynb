{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Парсим мемы в питоне: как обойти серверную блокировку. \n",
    "\n",
    "<img align=\"center\" src=\"pictures/2k17.jpg\" height=\"300\" width=\"300\"> \n",
    "Новогодние праздники - прекрасный повод попрокрастинировать в уютной домашней обстановке и вспомнить дорогие сердцу мемы, уходящие навсегда, как совесть Electronic Arts, вместе с 2k17. \n",
    "\n",
    "Однако даже обильно сдобренная салатами совесть иногда просыпалась и требовала хоть немного взять себя в руки и заняться полезной деятельностью. Поэтому мы совместили приятное с полезным и на примере любимых мемов посмотрели, как можно спарсить себе небольшую базу данных, попутно обходя всевозможные блокировки, ловушки и ограничения, расставленные сервером на нашем пути. Всех заинтересованных любезно приглашаем под кат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Meme Parsing: how to avoid server bans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning, Econometrics, Statistics, and many other Data Science fields are searching for patterns and correlations. Each day brave analysts-inquisitors are torturing data trying to find answers to life, the universe, and everything. Obviously, before torture happens inquisitor has to get a victim, preferably [relevant, clear, and consistent](https://en.wikipedia.org/wiki/Data_model). This article is all about how to get/grab/parse data for your research purposes. \n",
    "\n",
    "Our motto is a famous quote by Captain Jack Sparrow: \"Take what you can. Give nothing back\". Sometimes meme parsing requires a bit of pirating. However, we are no pirates, just peaceful data-gatherers, and we strongly dissaprove any illegal actions agains honest server owners. \n",
    "\n",
    "\n",
    "<img align=\"center\" src=\"pictures/captain.jpg\n",
    "\" height=\"300\" width=\"300\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Breaking into the memevault\n",
    "\n",
    "## 1.1. What we want to get\n",
    "\n",
    "So, we want to parse [knowyourmeme.com](http://knowyourmeme.com) and get lots of variables:\n",
    "\n",
    "- **Name** – name of the meme,\n",
    "- **Origin_year** – year when it first appeared,\n",
    "- **Views** – number of views on the site,\n",
    "- **About** – text description of meme,\n",
    "- ** and many others** \n",
    "\n",
    "Moreover, we want to get our data without all that: \n",
    "\n",
    "<img align=\"center\" src=\"pictures/2.png\" height=\"600\" width=\"600\">\n",
    "\n",
    "After downloading and cleaning of data we will be able to build interesting models. For example, we can try to predict meme popularity based on its parameters. But fow now let's focus on a bunch of definitions:\n",
    "\n",
    "* **Parser** — script that grabs data from a website\n",
    "* **Crawler** — part of a parser that crawls through weblinks\n",
    "* **Crawling** — process of crawling through weblinks\n",
    "* **Scrapping** — collecting data from webpages \n",
    "* **Parsing** — crawling + scraping combined!\n",
    "\n",
    "## 1.2. What's HTML\n",
    "\n",
    "**HTML (HyperText Markup Language)** - is just your friendly neighborhood markup language, just like Markdown or LaTeX. It is a standard for various websites. If you open any given website, click your right mouse button and move to `View page source` you will see the HTML-skeleton of this site. \n",
    "\n",
    "You may notice that an HTML-page is nothing more but a bunch of nested **tags**. For example, some of the most common tags are:\n",
    "\n",
    "- `<title>` – title of the page\n",
    "- `<h1>…<h6>` – headers of different levels\n",
    "- `<p>` – paragraph\n",
    "- `<div>` – division or a section in an HTML document\n",
    "- `<table>` – drawing tables\n",
    "- `<tr>` – row separator in the table\n",
    "- `<td>` – column separator in the table\n",
    "- `<b>` – bold text\n",
    "\n",
    "Usually `<...>` opens a tag and `</...>` closes it. Everything in between follows the rules, defined by the tag. For example, evrything in between of `<p>` and `</p>` is a separate paragraph. \n",
    "\n",
    "Tags form a special tree with a root in `<html>` tag and divide the page into different logical pieces. Each tag can have its own children - tags that are nested inside of it, and its own parents. \n",
    "\n",
    "A typical HTML-tree can look like that:\n",
    "\n",
    "```html\n",
    "    <html>\n",
    "    <head> Header </head>\n",
    "    <body>\n",
    "        <div> \n",
    "            First part of text with its own properties\n",
    "        </div>\n",
    "        <div>\n",
    "            Second part of text\n",
    "                <b>\n",
    "                    Third, bold part\n",
    "                </b>\n",
    "        </div>\n",
    "        Fourth part   \n",
    "    </body>\n",
    "    </html> \n",
    "```\n",
    "\n",
    "<img align=\"center\" src=\"pictures/tree.png\" height=\"500\" width=\"500\"> \n",
    "\n",
    "It's possible to work with HTML document as with a text file or as with a tree. Walking through the HTML tree is called parsing. We will be looking for specific nodes in this tree and grap data contained in them. \n",
    "\n",
    "It's dangerous to parse manually, wo here are some tools:\n",
    "\n",
    "- [CSS-selector](https://www.w3schools.com/cssref/css_selectors.asp) (when we are searhing for the element using a key-value pair)\n",
    "- [XPath](https://www.w3schools.com/xml/xpath_intro.asp) (when we point to either a full or relative position of the element in the tree: i.e. /html/body/div[1]/div[3]/div/div[2]/div)\n",
    "- Many other libraries for many other languages, for example, BeutifulSoup for Python, which we will be using in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Our first request\n",
    "\n",
    "We can access webpages with the help of the `requests` module. Let's load it altogether with a couple other useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests      # Sending requests\n",
    "import numpy as np   # Matrices, vectors, linear algebra\n",
    "import pandas as pd  # Tables and data manipulations\n",
    "import time          # Manipulating the time-space continuum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin our nobel research adventure we need to collect data on each meme from its corresponding page. But before that we need to get all those page addresses. So we open the main page with all the memes which looks like this:\n",
    "\n",
    "<img align=\"center\" src=\"pictures/memes_main_new.png\" height=\"500\" width=\"500\"> \n",
    "\n",
    "That is where we're going to start parsing. Let's save main page address in a variable `page_link` and open it with `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_link = 'http://knowyourmeme.com/memes/all/page/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(page_link)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the first problem! Quick search in [the main knowledge sourse](https://en.wikipedia.org/wiki/HTTP_403) tells us that the server returns 403-rd error if it's accessible and ready to process our requests but for some personal reasons refuses to do so. \n",
    "\n",
    "<img align=\"center\" src=\"pictures/403.jpg\" height=\"350\" width=\"350\"> \n",
    "\n",
    "Let's find out why. First we check how final request looked like when it went to the server, mode precisely - how our User-Agent looked like in the eyes of the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Agent: python-requests/2.18.4\n",
      "Accept-Encoding: gzip, deflate\n",
      "Accept: */*\n",
      "Connection: keep-alive\n"
     ]
    }
   ],
   "source": [
    "for key, value in response.request.headers.items():\n",
    "    print(key+\": \"+value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like unequivocally we let the server know we are using python, working with `requests` libriry, version 2.18.4. That might have looked just a little bit suspicious for a server, so it decided to reject us for good. To have some frame of reference, here is how a normal human request-header looks like:\n",
    "\n",
    "<img align=\"center\" src=\"pictures/good_headers.png\" height=\"800\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже, мы недвусмысленно дали понять серверу, что мы сидим на питоне и используем библиотеку requests под версией 2.14.2. Скорее всего, это вызвало у сервера некоторые подозрения относительно наших благих намерений и он решил нас безжалостно отвергнуть. Для сравнения, можно посмотреть, как выглядят request-headers у здорового человека:\n",
    "\n",
    "<img align=\"center\" src=\"pictures/good_headers.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "Очевидно, что нашему скромному запросу не тягаться с таким обилием мета-информации, которое передается при запросе из обычного браузера. К счастью, никто нам не мешает притвориться человечными и пустить пыль в глаза сервера при помощи генерации фейкового юзер-агента. Библиотек, которые справляются с такой задачей, существует очень и очень много, лично мне больше всего нравится [`fake-useragent`](https://pypi.python.org/pypi/fake-useragent). При вызове метода из различных кусочков будет генерироваться рандомное сочетание операционной системы, спецификаций и версии браузера, которые можно передавать в запрос:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подгрузим один из методов этой библиотеки\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UserAgent().chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(page_link, headers={'User-Agent': UserAgent().chrome})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"pictures/200.jpeg\" height=\"350\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замечательно, наша небольшая маскировка сработала и обманутый сервер покорно выдал благословенный 200 ответ — соединение установлено и данные получены, всё чудесно! Посмотрим, что же все-таки мы получили."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html xmlns:fb=\\'https://www.facebook.com/2008/fbml\\' xmlns=\\'https://www.w3.org/1999/xhtml\\'>\\n<head>\\n<meta content=\\'text/html; charset=utf-8\\' http-equiv=\\'Content-Type\\'>\\n<script type=\"text/javascript\">window.NREUM||(NREUM={});NREUM.info={\"beacon\":\"bam.nr-data.net\",\"errorBeacon\":\"bam.nr-data.net\",\"licenseKey\":\"c1a6d52f38\",\"applicationID\":\"31165848\",\"transactionName\":\"dFdfRUpeWglTQB8GDUNKWFRLHlcJWg==\",\"queueTime\":0,\"applicationTime\":45,\"agent\":\"\"}</script>\\n<script type=\"text/javascript\">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e(\"handle\"),a=e(3),u=e(4),f=e(\"ee\").get(\"tracer'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит неудобоваримо, как насчет сварить из этого дела что-то покрасивее? Например, красивый суп."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Красивый суп\n",
    "\n",
    "<img align=\"center\" src=\"pictures/soup.jpg\" height=\"200\" width=\"200\"> \n",
    "\n",
    "Пакет **[bs4 , a.k.a BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)** (тут есть гиперссылка на лучшего друга человека — документацию) был назван в честь стишка про красивый суп из Алисы в стране чудес.\n",
    "\n",
    "Красивый суп — это совершенно волшебная библиотека, которая из сырого и необработанного HTML кода страницы выдаст вам структурированный массив данных, по которому очень удобно искать необходимые теги, классы, атрибуты, тексты и прочие элементы веб страниц.\n",
    "\n",
    "> Пакет под названием `BeautifulSoup` — скорее всего, не то, что нам нужно. Это третья версия (*Beautiful Soup 3*), а мы будем использовать четвертую. Нужно будет установить пакет `beautifulsoup4`. Чтобы было совсем весело, при импорте нужно указывать другое название пакета — `bs4`, а импортировать функцию под названием `BeautifulSoup`. В общем, сначала легко запутаться, но эти трудности нужно преодолеть.\n",
    "\n",
    "С необработанным XML кодом страницы пакет также работает (XML — это исковерканый и превращённый в диалект, с помощью своих команд, HTML). Для того, чтобы пакет корректно работал с XML разметкой, придётся в довесок ко всему нашему арсеналу установить пакет `xml`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передадим функции `BeautifulSoup` текст веб-страницы, которую мы недавно получили."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html,'html.parser') # В опции также можно указать lxml, \n",
    "                                         # если предварительно установить одноименный пакет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим что-то вот такое:\n",
    "    \n",
    "```\n",
    "<!DOCTYPE html>\n",
    "\n",
    "<html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:fb=\"http://www.facebook.com/2008/fbml\">\n",
    "<head>\n",
    "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
    "<script type=\"text/javascript\">window.NREUM||(NREUM={});NREUM.info={\"beacon\":\"bam.nr-data.net\",\"errorBeacon\":\"bam.nr-data.net\",\"licenseKey\":\"c1a6d52f38\",\"applicationID\":\"31165848\",\"transactionName\":\"dFdfRUpeWglTQB8GDUNKWFRLHkUNWUU=\",\"queueTime\":0,\"applicationTime\":24,\"agent\":\"\"}</script>\n",
    "<script type=\"text/javascript\">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e(\"handle\"),a=e(2),u=e(3),f=e(\"ee\").get(\"tracer\"),c=e(\"loader\"),s=NREUM;\"undefined\"==typeof window.newrelic&&(newrelic=s);var p=\n",
    "```\n",
    "\n",
    "Стало намного лучше, не правда ли? Что же лежит в переменной `soup`? Невнимательный пользователь, скорее всего, скажет,что ничего вообще не изменилось. Тем не менее, это не так. Теперь мы можем свободно бродить по HTML-дереву страницы, искать детей, родителей и вытаскивать их! \n",
    "\n",
    "Например, можно бродить по вершинам, указывая путь из тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>All Entries | Know Your Meme</title>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.html.head.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно вытащить из того места, куда мы забрели, текст с помощью метода `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All Entries | Know Your Meme'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.html.head.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, зная адрес элемента, мы сразу можем найти его. Например, можно сделать это по классу. Следующая команда должна найти элемент, который лежит внутри тега `a` и имеет класс `photo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"photo left\" href=\"/memes/anime-was-a-mistake\" target=\"_self\"><img alt='The Fifth Anniversary of \"Anime Was a Mistake\"' data-src=\"https://i.kym-cdn.com/featured_items/icons/wide/000/009/001/animemistake.jpg\" height=\"112\" src=\"https://s.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title='The Fifth Anniversary of \"Anime Was a Mistake\"' width=\"198\"/> <div class=\"info abs\"> <div class=\"c\"> The Fifth Anniversary of \"Anime Was a Mistake\" </div> </div> </a>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = soup.find('a', attrs = {'class':'photo'})\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, вопреки нашим ожиданиям, вытащенный объект имеет класс `\"photo left\"`. Оказывается, `BeautifulSoup4` расценивает аттрибуты `class` как набор отдельных значений, поэтому `\"photo left\"` для библиотеки равносильно `[\"photo\", \"left\"]`, а указанное нами значение этого класса `\"photo\"` входит в этот список. Чтобы избежать такой неприятной ситуации и проходов по ненужным нам ссылкам, придется воспользоваться собственной функцией и задать строгое соответствие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"photo\" href=\"/memes/social-media-influencer\"><img alt=\"Social Media Influencer\" data-src=\"https://i.kym-cdn.com/entries/icons/medium/000/028/344/social_media_influencer.jpg\" src=\"https://s.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title=\"Social Media Influencer\"/> <div class=\"entry-labels\"> <span class=\"label label-submission\"> Submission </span> </div> </a>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = soup.find(lambda tag: tag.name == 'a' and tag.get('class') == ['photo'])\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученный после поиска объект также обладает структурой bs4. Поэтому можно продолжить искать нужные нам объекты уже в нём! Вытащим ссылку на этот мем. Сделать это можно по атрибуту `href`, в котором лежит наша ссылка. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/memes/social-media-influencer'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.attrs['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что после всех этих безумных преобразований у данных поменялся тип. Теперь они `str`. Это означет, что с ними можно работать как с текстом и пускать в ход для отсеивания лишней информации регулярные выражения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип данных до вытаскивания ссылки: <class 'bs4.element.Tag'>\n",
      "Тип данных после вытаскивания ссылки: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Тип данных до вытаскивания ссылки:\", type(obj))\n",
    "print(\"Тип данных после вытаскивания ссылки:\", type(obj.attrs['href']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если несколько элементов на странице обладают указанным адресом, то метод `find` вернёт только самый первый.  Чтобы найти все элементы с таким адресом, нужно использовать метод `findAll`, и на выход будет выдан список. Таким образом, мы можем получить одним поиском сразу все объекты, содержащие ссылки на страницы с мемами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"photo\" href=\"/memes/social-media-influencer\"><img alt=\"Social Media Influencer\" data-src=\"https://i.kym-cdn.com/entries/icons/medium/000/028/344/social_media_influencer.jpg\" src=\"https://s.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title=\"Social Media Influencer\"/> <div class=\"entry-labels\"> <span class=\"label label-submission\"> Submission </span> </div> </a>,\n",
       " <a class=\"photo\" href=\"/memes/subcultures/metroid-prime-4\"><img alt=\"Metroid Prime 4\" data-src=\"https://i.kym-cdn.com/entries/icons/medium/000/028/343/metroid-prime-4-1144404.jpeg\" src=\"https://s.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title=\"Metroid Prime 4\"/> <div class=\"entry-labels\"> <span class=\"label label-submission\"> Submission </span> <span class=\"label\" style=\"background: #5d1cb1; color: white;\">Subculture</span> </div> </a>,\n",
       " <a class=\"photo\" href=\"/memes/subcultures/leaving-neverland\"><img alt=\"Leaving Neverland\" data-src=\"https://i.kym-cdn.com/entries/icons/medium/000/028/342/jackson.jpg\" src=\"https://s.kym-cdn.com/assets/blank-b3f96f160b75b1b49b426754ba188fe8.gif\" title=\"Leaving Neverland\"/> <div class=\"entry-labels\"> <span class=\"label label-submission\"> Submission </span> <span class=\"label\" style=\"background: #5d1cb1; color: white;\">Subculture</span> <span class=\"label label-nsfw\"> NSFW </span> </div> </a>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_links = soup.findAll(lambda tag: tag.name == 'a' and tag.get('class') == ['photo'])\n",
    "meme_links[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось очистить полученный список от мусора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_links = [link.attrs['href'] for link in meme_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/memes/social-media-influencer',\n",
       " '/memes/subcultures/metroid-prime-4',\n",
       " '/memes/subcultures/leaving-neverland',\n",
       " '/memes/subcultures/genlock',\n",
       " '/memes/gordon-ramseys-hot-ones-interview',\n",
       " '/memes/brock-lesnar-at-the-2019-royal-rumble',\n",
       " '/memes/events/2019-buzzfeed-and-huffpost-layoffs',\n",
       " '/memes/pickle-chin',\n",
       " '/memes/zaddy',\n",
       " '/memes/subcultures/meme-review']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_links[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meme_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готово, получили ровно 16 ссылок по числу мемов на одной странице поиска. \n",
    "\n",
    "Хорошо, то, что можно искать элемент по его адресу, конечно же, круто, но откуда взять этот адрес? Можно установить для своего браузера какую-нибудь утилиту, позволяющую вытаскивать со страницы нужные теги, например, [selectorgadget.](http://selectorgadget.com/)\n",
    "\n",
    "Тем не менее, этот путь не подходит для истинного самурая. Для последователей бусидо есть другой способ — искать теги для каждого нужного нам элемента вручную. Для этого придётся жать правой кнопкой мышки по окну браузера и тыкать кнопку **Inspect**. После всех этих манипуляций браузер будет выглядеть как-то вот так: \n",
    "\n",
    "<img align=\"center\" src=\"pictures/memes_inspection.png\" height=\"800\" width=\"800\"> \n",
    "\n",
    "Выскочивший кусок html, в котором находится адрес выбранного вами объекта, можно смело копировать в код и наслаждаться своей брутальностью. \n",
    "\n",
    "Остался последний момент. Когда мы скачаем все мемы с текущей страницы, нам нужно будет каким-то образом забраться на соседнюю. На сайте это можно делать просто пролистывая страницу с мемами вниз, javascript-функции подтянут новые мемы на текущее окно, но сейчас трогать эти функции не хочется.\n",
    "\n",
    "Обычно, все параметры, которые мы устанавливаем на сайте для поиска, отображаются на структуре хрефа. Мемы не являются исключением. Если мы хотим получить первую порцию мемов, мы должны будем обратиться к сайту по ссылке \n",
    "\n",
    "                `http://knowyourmeme.com/memes/all/page/1`\n",
    "\n",
    "\n",
    "Если мы захотим получить вторую поцию с шестнадцатью мемами, нам придётся немного видоизменить ссылку, а именно заменить номер страницы на 2.\n",
    "\n",
    "\n",
    "                `http://knowyourmeme.com/memes/all/page/2`\n",
    " \n",
    "Таким незатейливым образом мы сможем пройтись по всем страницам и ограбить мемохранилище. \n",
    "\n",
    "Наконец, обернем в красивую функцию все-все манипуляции, проделанные выше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getPageLinks(page_number):\n",
    "    \"\"\"\n",
    "        Возвращает список ссылок на мемы, полученный с текущей страницы\n",
    "        \n",
    "        page_number: int/string\n",
    "            номер страницы для парсинга\n",
    "            \n",
    "    \"\"\"\n",
    "    # составляем ссылку на страницу поиска\n",
    "    page_link = 'http://knowyourmeme.com/memes/all/page/{}'.format(page_number)\n",
    "    \n",
    "    # запрашиваем данные по ней\n",
    "    response = requests.get(page_link, headers={'User-Agent': UserAgent().chrome})\n",
    "    \n",
    "    if not response.ok:\n",
    "        # если сервер нам отказал, вернем пустой лист для текущей страницы\n",
    "        return [] \n",
    "    \n",
    "    # получаем содержимое страницы и переводим в суп\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # наконец, ищем ссылки на мемы и очищаем их от ненужных тэгов\n",
    "    meme_links = soup.findAll(lambda tag: tag.name == 'a' and tag.get('class') == ['photo'])\n",
    "    meme_links = ['http://knowyourmeme.com' + link.attrs['href'] for link in meme_links]\n",
    "    \n",
    "    return meme_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем функцию и убедимся, что всё хорошо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://knowyourmeme.com/memes/social-media-influencer',\n",
       " 'http://knowyourmeme.com/memes/subcultures/metroid-prime-4']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_links = getPageLinks(1)\n",
    "meme_links[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://knowyourmeme.com/memes/viktor-rolf-meme-gowns',\n",
       " 'http://knowyourmeme.com/memes/uncle']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_links = getPageLinks(2)\n",
    "meme_links[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, функция работает и теперь мы теоретически можем достать ссылки на все $17171$ мем, для чего нам придется пройтись по $\\frac{17171}{16} \\approx 1074$ страницам. Прежде чем расстраивать сервер таким количеством запросов, посмотрим, как доставать всю необходимую информацию о конкретном меме. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Финальная подготовка к грабежу\n",
    "\n",
    "По аналогии со ссылками можно вытащить что угодно. Для этого надо сделать несколько шагов: \n",
    "\n",
    "1. Открываем страничку с мемом\n",
    "2. Находим любым способом тег для нужной нам информации\n",
    "3. Пихаем всё это в красивый суп\n",
    "4. ......\n",
    "5. Profit \n",
    "\n",
    "Для закрепления информации в голове любознательного читателя, вытащим число просмотров мема.\n",
    "\n",
    "А в качестве примера возьмем самый популярный на этом сайте мем - Doge, набравший более 12 миллионов просмотров по состоянию на 1 января 2018 года. \n",
    "\n",
    "Сама страница, с которой мы будем доставать дорогую нашему исследовательскому сердцу информацию выглядит следуюшим образом:\n",
    "\n",
    "<img align=\"center\" src=\"pictures/doge_main.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "Как и прежде, для начала сохраним ссылку на страницу в переменную и вытащим по ней контент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_page = 'http://knowyourmeme.com/memes/doge'\n",
    "\n",
    "response = requests.get(meme_page, headers={'User-Agent': UserAgent().chrome})\n",
    "\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как можно вытащить статистику просмотров, комментариев, а также числа загруженных видео и фото, связанных с нашим мемов. Всё это добро хранится справа вверху под тэгами `\"dd\"` и с классами  `\"views\"`, `\"videos\"`, `\"photos\"` и `\"comments\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dd class=\"views\" title=\"12,803,434 Views\">\n",
       "<a href=\"/memes/doge\" rel=\"nofollow\">12,803,434</a>\n",
       "</dd>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = soup.find('dd', attrs={'class':'views'})\n",
    "views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12,803,434'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = views.find('a').text\n",
    "views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12803434"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = int(views.replace(',', ''))\n",
    "views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова запихнём всё это в небольшую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getStats(soup, stats):\n",
    "    \"\"\"\n",
    "        Возвращает очищенное число просмотров/коментариев/...\n",
    "        \n",
    "        soup: объект bs4.BeautifulSoup \n",
    "            суп текущей страницы\n",
    "            \n",
    "        stats: string\n",
    "            views/videos/photos/comments\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    obj = soup.find('dd', attrs={'class':stats})\n",
    "    obj = obj.find('a').text\n",
    "    obj = int(obj.replace(',', ''))\n",
    "    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё готово! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Просмотры: 12803434\n",
      "Видео: 62\n",
      "Фото: 1649\n",
      "Комментарии: 916\n"
     ]
    }
   ],
   "source": [
    "views = getStats(soup, stats='views')\n",
    "videos = getStats(soup, stats='videos')\n",
    "photos = getStats(soup, stats='photos')\n",
    "comments = getStats(soup, stats='comments')\n",
    "\n",
    "print(\"Просмотры: {}\\nВидео: {}\\nФото: {}\\nКомментарии: {}\".format(views, videos, photos, comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще из интересного и исследовательского —  достанем дату и время добавления мема. Если посмотреть на страницу в браузере, можно подумать, что максимум информации, который мы можем вытащить - это число лет, прошедших с момента публикации —  `Added 4 years ago by NovaXP`. Однако мы так просто сдаваться не будем, полезем в кишки html и откопаем там кусок, ответственный за эту надпись:\n",
    "\n",
    "<img align=\"center\" src=\"pictures/no_shit.jpg\" height=\"300\" width=\"300\"> \n",
    "\n",
    "<img align=\"center\" src=\"pictures/html_time_ago.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "Ага! Вот и подробности по дате добавления, с точностью до минуты. Элементарно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-10-23T01:43:59-04:00'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = soup.find('abbr', attrs={'class':'timeago'}).attrs['title']\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле, парсеры — дело непредсказуемое. Часто страницы, которые мы парсим, имеют очень неоднородну структуру. Например, если мы парсим мемы, на части страниц может быть указано описание, а на части нет. Как только код впервые натыкается на отсутствие описания, он выдаёт ошибку и останавливается. Чтобы нормально собрать все данные, приходится прописывать исключения. Вроде бы, хранилище мемов хорошо оборудовано и никаких внештатных ситуаций происходить не должно. Тем не менее, очень не хочется проснуться утром и увидеть, что код сделал 20 итераций, нарвался на ошибку и отрубился.  Чтобы такого не произошло, можно, например, использовать конструкцию `try - except` и просто обрабатывать неугодные нам ошибки. Про исключения можно почитать [на просторах интернета](https://pythonworld.ru/tipy-dannyx-v-python/isklyucheniya-v-python-konstrukciya-try-except-dlya-obrabotki-isklyuchenij.html). В нашем же случае до ошибки можно и не доводить, а предварительно проверять, есть ли необходимый элемент на странице или нет при помощи обычного `if - else`, и уже после этого пытаться его распарсить.\n",
    "\n",
    "Например, мы хотим вытащить статус мема, для этого найдем окружающие его тэги:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dd>\n",
       "Confirmed\n",
       "</dd>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties = soup.find('aside', attrs={'class':'left'})\n",
    "meme_status = properties.find(\"dd\")\n",
    "meme_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше нужно вытащить из тэгов текст, а после обрубить все лишние пробелы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Confirmed'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_status.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, если неожиданно выяснится, что у мема нет статуса, метод `find` вернёт пустоту. Метод `text`, в свою очередь, не сможет найти в тэгах текст и выдаст ошибку. Чтобы обезопасить себя от таких пустот, можно прописать исключение или `if - else`. Так как в текущем меме статус все-таки есть, нарочно зададим его как пустой объект, чтобы проверить, что ошибка поймается в обоих случаях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception\n",
      "Empty\n"
     ]
    }
   ],
   "source": [
    "# Делай раз! Ищем статус мема, но не находим его\n",
    "meme_status = None\n",
    "\n",
    "# Делай два! Пытаемся вытащить его...\n",
    "\n",
    "# ... с исключениями\n",
    "try:\n",
    "    print(meme_status.text.strip()) \n",
    "# Ежели возникает ошибка, статус не найден, выдаём пустоту.\n",
    "except:\n",
    "    print(\"Exception\")\n",
    "    \n",
    "    \n",
    "# ... с проверкой на пустой элемент\n",
    "if meme_status:\n",
    "    print(meme_status.text.strip())\n",
    "else:\n",
    "    print(\"Empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой код позволяет обезопасить себя от ошибок в коде. В данном случае, мы можем переписать всю конструкцию с `if - else` в виде одной удобной строки. Эта строка проверит полон ли респонса `meme_status` и ежели нет, то выдаст пустоту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed\n"
     ]
    }
   ],
   "source": [
    "# снова найдем настоящий статус\n",
    "properties = soup.find('aside', attrs={'class':'left'})\n",
    "meme_status = properties.find(\"dd\")\n",
    "\n",
    "meme_status = \"\" if not meme_status else meme_status.text.strip()\n",
    "print(meme_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По аналогии можно вытащить всю остальную информацию со страницы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getProperties(soup):\n",
    "    \"\"\"\n",
    "        Возвращает список (tuple) с названием, статусом, типом, \n",
    "        годом и местом происхождения и тэгами\n",
    "        \n",
    "        soup: объект bs4.BeautifulSoup \n",
    "            суп текущей страницы\n",
    "    \n",
    "    \"\"\"\n",
    "    # название - идёт с самым большим заголовком h1, легко найти\n",
    "    meme_name = soup.find('section', attrs={'class':'info'}).find('h1').text.strip()\n",
    "    \n",
    "    # достаём все данные справа от картинки \n",
    "    properties = soup.find('aside', attrs={'class':'left'})\n",
    "    \n",
    "    # статус идет первым - можно не уточнять класс\n",
    "    meme_status = properties.find(\"dd\")\n",
    "    # oneliner, заменяющий try-except: если тэга нет в properties, вернётся объект NoneType,\n",
    "    # у которого аттрибут text отсутствует, и в этом случае он заменится на пустую строку\n",
    "    meme_status = \"\" if not meme_status else meme_status.text.strip()\n",
    "    \n",
    "    # тип мема - обладает уникальным классом\n",
    "    meme_type = properties.find('a', attrs={'class':'entry-type-link'})\n",
    "    meme_type = \"\" if not meme_type else meme_type.text \n",
    "    \n",
    "    # год происхождения первоисточника можно найти после заголовка Year, \n",
    "    # находим заголовок, определяем родителя и ищем следущего ребенка - наш раздел\n",
    "    meme_origin_year = properties.find(text='\\nYear\\n')\n",
    "    meme_origin_year = \"\" if not meme_origin_year else meme_origin_year.parent.find_next()\n",
    "    meme_origin_year = meme_origin_year.text.strip()\n",
    "    \n",
    "    # сам первоисточник\n",
    "    meme_origin_place = properties.find('dd', attrs={'class':'entry_origin_link'})\n",
    "    meme_origin_place = \"\" if not meme_origin_place else meme_origin_place.text.strip()\n",
    "    \n",
    "    # тэги, связанные с мемом\n",
    "    meme_tags = properties.find('dl', attrs={'id':'entry_tags'}).find('dd')\n",
    "    meme_tags = \"\" if not meme_tags else meme_tags.text.strip()\n",
    "    \n",
    "    return meme_name, meme_status, meme_type, meme_origin_year, meme_origin_place, meme_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Doge',\n",
       " 'Confirmed',\n",
       " 'Animal',\n",
       " '2013',\n",
       " 'Tumblr',\n",
       " 'animal, dog, shiba inu, shibe, such doge, super shibe, japanese, super, tumblr, much, very, many, comic sans, photoshop meme, such, shiba, shibe doge, doges, dogges, reddit, comic sans ms, tumblr meme, hacked, bitcoin, dogecoin, shitposting, stare, canine')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getProperties(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свойства мема собрали. Теперь собираем по аналогии его текстовое описание. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getText(soup):\n",
    "    \"\"\"\n",
    "        Возвращает текстовые описания мема\n",
    "        \n",
    "        soup: объект bs4.BeautifulSoup \n",
    "            суп текущей страницы\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # достаём все тексты под картинкой\n",
    "    body = soup.find('section', attrs={'class':'bodycopy'})\n",
    "    \n",
    "    # раздел about (если он есть), должен идти первым, берем его без уточнения класса\n",
    "    meme_about = body.find('p')\n",
    "    meme_about = \"\" if not meme_about else meme_about.text\n",
    "    \n",
    "    # раздел origin можно найти после заголовка Origin или History, \n",
    "    # находим заголовок, определяем родителя и ищем следущего ребенка - наш раздел\n",
    "    meme_origin = body.find(text='Origin') or body.find(text='History')\n",
    "    meme_origin = \"\" if not meme_origin else meme_origin.parent.find_next().text\n",
    "    \n",
    "    # весь остальной текст (если он есть) можно запихнуть в одно текстовое поле\n",
    "    if body.text:\n",
    "        other_text = body.text.strip().split('\\n')[4:]\n",
    "        other_text = \" \".join(other_text).strip()\n",
    "    else:\n",
    "        other_text = \"\"\n",
    "        \n",
    "    return meme_about, meme_origin, other_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "О чем мем:\n",
      "Doge is a slang term for \"dog\" that is primarily associated with pictures of Shiba Inus (nicknamed \"Shibe\") and internal monologue captions on Tumblr. These photos may be photoshopped to change the dog's face or captioned with interior monologues in Comic Sans font.\n",
      "\n",
      "Происхождение:\n",
      "The use of the misspelled word \"doge\" to refer to a dog dates back to June 24th, 2005, when it was mentioned in an episode of Homestar Runner's puppet show. In the episode titled \"Biz Cas Fri 1\"[2], Homestar calls Strong Bad his \"d-o-g-e\" while trying to distract him from his work.\n",
      "\n",
      "Остальной текст:\n",
      "Identity On February 23rd, 2010, Japanese kindergarten teacher Atsuko Sato posted several photos of her rescue-adopted Shiba Inu dog Kabosu to her personal blog.[38] Among the photos included a peculi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meme_about, meme_origin, other_text = getText(soup)\n",
    "\n",
    "print(\"О чем мем:\\n{}\\n\\nПроисхождение:\\n{}\\n\\nОстальной текст:\\n{}...\\n\"\\\n",
    "      .format(meme_about, meme_origin, other_text[:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, создадим функцию, возвращающую всю информацию по текущему мему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getMemeData(meme_page):\n",
    "    \"\"\"\n",
    "        Запрашивает данные по странице, возвращает обработанный словарь с данными\n",
    "        \n",
    "        meme_page: string\n",
    "            ссылка на страницу с мемом\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # запрашиваем данные по ссылке\n",
    "    response = requests.get(meme_page, headers={'User-Agent': UserAgent().chrome})\n",
    "    \n",
    "    if not response.ok:\n",
    "        # если сервер нам отказал, вернем статус ошибки \n",
    "        return response.status_code\n",
    "    \n",
    "    # получаем содержимое страницы и переводим в суп\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "    # используя ранее написанные функции парсим информацию\n",
    "    views = getStats(soup=soup, stats='views')\n",
    "    videos = getStats(soup=soup, stats='videos')\n",
    "    photos = getStats(soup=soup, stats='photos')\n",
    "    comments = getStats(soup=soup, stats='comments')\n",
    "\n",
    "    # дата\n",
    "    date = soup.find('abbr', attrs={'class':'timeago'}).attrs['title']\n",
    "\n",
    "    # имя, статус, и т.д.\n",
    "    meme_name, meme_status, meme_type, meme_origin_year, meme_origin_place, meme_tags =\\\n",
    "    getProperties(soup=soup)\n",
    "\n",
    "    # текстовые поля\n",
    "    meme_about, meme_origin, other_text = getText(soup=soup)\n",
    "\n",
    "    # составляем словарь, в котором будут хранится все полученные и обработанные данные\n",
    "    data_row = {\"name\":meme_name, \"status\":meme_status, \n",
    "                \"type\":meme_type, \"origin_year\":meme_origin_year, \n",
    "                \"origin_place\":meme_origin_place,\n",
    "                \"date_added\":date, \"views\":views, \n",
    "                \"videos\":videos, \"photos\":photos, \"comments\":comments, \"tags\":meme_tags,\n",
    "                \"about\":meme_about, \"origin\":meme_origin, \"other_text\":other_text}\n",
    "\n",
    "    return data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row = getMemeData('http://knowyourmeme.com/memes/doge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь подготовим табличку, чтобы в неё записывать всё ~~награбленные~~ честно полученные данные, добавим в неё первую полученную строку и полюбуемся на результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(columns=['name', 'status', 'type', 'origin_year', 'origin_place',\n",
    "                                 'date_added', 'views', 'videos', 'photos', 'comments', \n",
    "                                 'tags', 'about', 'origin', 'other_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.append(data_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>status</th>\n",
       "      <th>type</th>\n",
       "      <th>origin_year</th>\n",
       "      <th>origin_place</th>\n",
       "      <th>date_added</th>\n",
       "      <th>views</th>\n",
       "      <th>videos</th>\n",
       "      <th>photos</th>\n",
       "      <th>comments</th>\n",
       "      <th>tags</th>\n",
       "      <th>about</th>\n",
       "      <th>origin</th>\n",
       "      <th>other_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doge</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>Animal</td>\n",
       "      <td>2013</td>\n",
       "      <td>Tumblr</td>\n",
       "      <td>2018-10-23T01:43:59-04:00</td>\n",
       "      <td>12803434</td>\n",
       "      <td>62</td>\n",
       "      <td>1649</td>\n",
       "      <td>916</td>\n",
       "      <td>animal, dog, shiba inu, shibe, such doge, supe...</td>\n",
       "      <td>Doge is a slang term for \"dog\" that is primari...</td>\n",
       "      <td>The use of the misspelled word \"doge\" to refer...</td>\n",
       "      <td>Identity On February 23rd, 2010, Japanese kind...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name     status    type origin_year origin_place  \\\n",
       "0  Doge  Confirmed  Animal        2013       Tumblr   \n",
       "\n",
       "                  date_added     views videos photos comments  \\\n",
       "0  2018-10-23T01:43:59-04:00  12803434     62   1649      916   \n",
       "\n",
       "                                                tags  \\\n",
       "0  animal, dog, shiba inu, shibe, such doge, supe...   \n",
       "\n",
       "                                               about  \\\n",
       "0  Doge is a slang term for \"dog\" that is primari...   \n",
       "\n",
       "                                              origin  \\\n",
       "0  The use of the misspelled word \"doge\" to refer...   \n",
       "\n",
       "                                          other_text  \n",
       "0  Identity On February 23rd, 2010, Japanese kind...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый мем оказался в наших рукак. Еще раз убедимся что всё работает — пройдемся по списку из ссылок на мемы, полученных ранее в перменной `meme_links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for meme_link in meme_links:\n",
    "    data_row = getMemeData(meme_link)\n",
    "    final_df = final_df.append(data_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>status</th>\n",
       "      <th>type</th>\n",
       "      <th>origin_year</th>\n",
       "      <th>origin_place</th>\n",
       "      <th>date_added</th>\n",
       "      <th>views</th>\n",
       "      <th>videos</th>\n",
       "      <th>photos</th>\n",
       "      <th>comments</th>\n",
       "      <th>tags</th>\n",
       "      <th>about</th>\n",
       "      <th>origin</th>\n",
       "      <th>other_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doge</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>Animal</td>\n",
       "      <td>2013</td>\n",
       "      <td>Tumblr</td>\n",
       "      <td>2018-10-23T01:43:59-04:00</td>\n",
       "      <td>12803434</td>\n",
       "      <td>62</td>\n",
       "      <td>1649</td>\n",
       "      <td>916</td>\n",
       "      <td>animal, dog, shiba inu, shibe, such doge, supe...</td>\n",
       "      <td>Doge is a slang term for \"dog\" that is primari...</td>\n",
       "      <td>The use of the misspelled word \"doge\" to refer...</td>\n",
       "      <td>Identity On February 23rd, 2010, Japanese kind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Viktor &amp; Rolf Meme Gowns</td>\n",
       "      <td>Submission</td>\n",
       "      <td>Pop Culture Reference</td>\n",
       "      <td>2019</td>\n",
       "      <td>Viktor Horsting and Rolf Snoeren</td>\n",
       "      <td>2019-01-25T16:11:26-05:00</td>\n",
       "      <td>5126</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>viktor, rolf, couture, fashion, art, clothes, ...</td>\n",
       "      <td>Viktor &amp; Rolf Meme Gowns refers to a series of...</td>\n",
       "      <td>On January 23rd, 2019, designers Viktor &amp; Rolf...</td>\n",
       "      <td>Spread That day, fans of the collection expres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Uncle</td>\n",
       "      <td>Submission</td>\n",
       "      <td></td>\n",
       "      <td>2018</td>\n",
       "      <td>Red Dead Redemption</td>\n",
       "      <td>2019-01-25T15:43:40-05:00</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none...</td>\n",
       "      <td>Uncle is a major character in Rockstar Games' ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Slow Meme</td>\n",
       "      <td>Submission</td>\n",
       "      <td>Participatory Media</td>\n",
       "      <td>2019</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>2019-01-25T17:41:04-05:00</td>\n",
       "      <td>2136</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>painting, reddit, inception, droste effect, sw...</td>\n",
       "      <td>Slow Meme refers to a variation of the Droste ...</td>\n",
       "      <td>On January 16th, 2019, Redditor [1] Gaddafo po...</td>\n",
       "      <td>That day, Redditor[2] k__z posted a painting o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4Head</td>\n",
       "      <td>Submission</td>\n",
       "      <td>Emoticon</td>\n",
       "      <td>2016</td>\n",
       "      <td>Twitch</td>\n",
       "      <td>2019-01-25T14:49:43-05:00</td>\n",
       "      <td>1692</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>twitch, joke, cadberry, 3head</td>\n",
       "      <td></td>\n",
       "      <td>While the exact date of the emote's creation i...</td>\n",
       "      <td>Spread The emote has been one of Twitch's most...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name      status                   type origin_year  \\\n",
       "0                      Doge   Confirmed                 Animal        2013   \n",
       "1  Viktor & Rolf Meme Gowns  Submission  Pop Culture Reference        2019   \n",
       "2                     Uncle  Submission                               2018   \n",
       "3                 Slow Meme  Submission    Participatory Media        2019   \n",
       "4                     4Head  Submission               Emoticon        2016   \n",
       "\n",
       "                       origin_place                 date_added     views  \\\n",
       "0                            Tumblr  2018-10-23T01:43:59-04:00  12803434   \n",
       "1  Viktor Horsting and Rolf Snoeren  2019-01-25T16:11:26-05:00      5126   \n",
       "2               Red Dead Redemption  2019-01-25T15:43:40-05:00       175   \n",
       "3                            Reddit  2019-01-25T17:41:04-05:00      2136   \n",
       "4                            Twitch  2019-01-25T14:49:43-05:00      1692   \n",
       "\n",
       "  videos photos comments                                               tags  \\\n",
       "0     62   1649      916  animal, dog, shiba inu, shibe, such doge, supe...   \n",
       "1      1     13       13  viktor, rolf, couture, fashion, art, clothes, ...   \n",
       "2      0      0        0                                            none...   \n",
       "3      0     13        0  painting, reddit, inception, droste effect, sw...   \n",
       "4      0      3        3                      twitch, joke, cadberry, 3head   \n",
       "\n",
       "                                               about  \\\n",
       "0  Doge is a slang term for \"dog\" that is primari...   \n",
       "1  Viktor & Rolf Meme Gowns refers to a series of...   \n",
       "2  Uncle is a major character in Rockstar Games' ...   \n",
       "3  Slow Meme refers to a variation of the Droste ...   \n",
       "4                                                      \n",
       "\n",
       "                                              origin  \\\n",
       "0  The use of the misspelled word \"doge\" to refer...   \n",
       "1  On January 23rd, 2019, designers Viktor & Rolf...   \n",
       "2                                                      \n",
       "3  On January 16th, 2019, Redditor [1] Gaddafo po...   \n",
       "4  While the exact date of the emote's creation i...   \n",
       "\n",
       "                                          other_text  \n",
       "0  Identity On February 23rd, 2010, Japanese kind...  \n",
       "1  Spread That day, fans of the collection expres...  \n",
       "2                                                     \n",
       "3  That day, Redditor[2] k__z posted a painting o...  \n",
       "4  Spread The emote has been one of Twitch's most...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 14)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично! Всё работает, мемы качаются, данные наполняются и всё было бы хорошо, если бы не одно но — количество запросов, которое нам придётся сделать, чтобы всё получить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Прячемся от стражников\n",
    "\n",
    "## 2.1 Когда работающий код больше не работает\n",
    "\n",
    "Вот он! Тот самый момент абсолютного триумфа, когда код дописан и всё, что нам, мирным собирателям, остаётся — запустить наш код на одну ночку. Кажется, что через страсть мы преобрели силу. Запускаем наш код по всем $1075$ страницам с мемами. На всякий случай обернём наш цикл в `try-except`. Мало ли что там с этими мемами бывает. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Немного красивых циклов. При желании пакет можно отключить и \n",
    "# удалить команду tqdm_notebook из всех циклов\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "final_df = pd.DataFrame(columns=['name', 'status', 'type', 'origin_year', 'origin_place',\n",
    "                                 'date_added', 'views', 'videos', 'photos', 'comments', \n",
    "                                 'tags', 'about', 'origin', 'other_text'])\n",
    "\n",
    "for page_number in tqdm_notebook(range(1075), desc='Pages'):\n",
    "    # собрали хрефы с текущей страницы\n",
    "    meme_links = getPageLinks(page_number)  \n",
    "    for meme_link in tqdm_notebook(meme_links, desc='Memes', leave=False):\n",
    "        # иногда с первого раза страничка не парсится\n",
    "        for i in range(5):\n",
    "            try:\n",
    "                # пытаемся собрать по мему немного даты\n",
    "                data_row = getMemeData(meme_link)           \n",
    "                # и закидываем её в таблицу\n",
    "                final_df = final_df.append(data_row, ignore_index=True)  \n",
    "                # если всё получилось - выходим из внутреннего цикла\n",
    "                break\n",
    "            except:\n",
    "                # Иначе, пробуем еще несколько раз, пока не закончатся попытки\n",
    "                print('AHTUNG! parsing once again:', meme_link)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сон был прекрасным! Солнце только-только взошло из-за горизонта, мы уже бежим за компьютер смотреть мемы и видим, что огромное число мемов не скачалось.\n",
    "\n",
    "<img align=\"center\" src=\"pictures/work_or_not.jpg\" height=\"500\" width=\"500\">\n",
    "\n",
    "Конечно же, вполне естественной реакцией будет нажать на первую же ссылку, перейти в мемохранилище и увидеть, что нас забанили.\n",
    "\n",
    "<img align=\"center\" src=\"pictures/memes_ban.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "Все наши реквесты остались без респонсов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Тор - сын Одина\n",
    "\n",
    "Иногда серверу надоедает общаться с одним и тем же человеком, делающим кучу запросов и сервер банит его. К сожалению, не только у людей запас терпения ограничен.\n",
    "\n",
    "Приходится маскироваться. Для такой маскировки можно использовать разные способы, более того, один из них мы уже использовали, когда притворились человеком в нашем `request-header`. Для текущей же задачи, когда нас вероломно заблокировали по IP, нужно искать способы помощнее, чтобы иметь возможность этот IP менять. Конечно, как вариант можно было бы использовать прокси-сервера, тогда мы бы имели в запасе некоторое количество разных IP адресов, которые можно подставлять по мере \"забанивания\". Однако в этом подходе есть пара проблем: первая - нужно где-то раздобыть эти прокси, вторая - а что если ограниченного числа адресов нам не хватит и нужно больше?\n",
    "\n",
    "В таком случае лучше всего нам подойдёт [Tor](https://www.torproject.org/). Вопреки пропагандируемому мнению, Tor используется не только преступниками, педофилами и прочими нехорошими террористами. Это, мягко говоря, далеко не так, и мы, мирные собиратели данных, являемся тому подтверждением. Всем прелестям, связанным с работой Tor, можно было бы посвятить несколько больших статеек, что собственно говоря уже и сделано. Подробнее про это можно почитатать по следующим ссылкам: \n",
    "\n",
    "* [Как работает Tor](https://geektimes.ru/post/277578/)\n",
    "* [Методы анонимности в сети](https://habrahabr.ru/post/204266/)\n",
    "* [Прокси-сервер с помощью Tor](https://habrahabr.ru/company/etagi/blog/315002/)\n",
    "\n",
    "Мы же ограничимся только функциональной частью, а именно без углубления в детали опишем шаги, которые нужно предпринять для того, чтобы использовать возможности Tor для обхода блокировки. Для начала полюбуемся на свой ip-адрес. Для этого сделаем get-запрос к сайту, который возвращяет наш IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIP():\n",
    "    ip = requests.get('http://checkip.dyndns.org').content\n",
    "    soup = BeautifulSoup(ip, 'html.parser')\n",
    "    print(soup.find('body').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current IP Address: 82.130.19.198\n"
     ]
    }
   ],
   "source": [
    "checkIP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменить свой ip через Tor можно двумя путями. Простой - через браузер, а сложный - через небольшие махинации с настройками.\n",
    "\n",
    "Скачаем браузерный [tor,](https://www.torproject.org/download/download) чтобы лёгкий путь был совсем прост. Для сложного пути в довесок к браузеру поставим tor через консоль. \n",
    "- Linux - нам поможет команда `apt-get install tor`, \n",
    "- Mac - сделаем это [в рамках brew](https://www.torproject.org/docs/tor-doc-osx.html.en), `brew install tor`. \n",
    "- Windows - нам поможет установка другой операционной системы. \n",
    "\n",
    "## 2.3 Путь первый \n",
    "\n",
    "Теперь запускаем свежескачанный браузер и оставляем его открытым.  Менять ip нам поможет библиотека `PySocks`. Конечно же, её нужно установить, скопировав в терминал `pip3 install PySocks`.  \n",
    "\n",
    "Браузер тора по умолчанию использует порт номер 9150. В питоне при помощи библиотек socks и socket можно задать дефолтный порт для подключения. В результате текущая сессия будет использовать именно этот порт при отправке любого запроса, а значит – запросы будут посылаться из-под запущенного тора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socks\n",
    "import socket\n",
    "socks.set_default_proxy(socks.SOCKS5, \"localhost\", 9150)\n",
    "socket.socket = socks.socksocket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на свой новый ip-aдрес. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current IP Address: 199.249.230.88\n"
     ]
    }
   ],
   "source": [
    "checkIP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При сопутствующем желании, [можно выяснить](http://ru.smart-ip.net/geoip) из какой страны в данный момент мы сидим в интернете.\n",
    "\n",
    "Ого...бывший член Европейского союза!\n",
    "\n",
    "<img align=\"center\" src=\"pictures/britain.png\" height=\"250\" width=\"250\"> \n",
    "\n",
    "Попробуем обратиться к мемохранилищу с нового ip-адреса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Doge\n",
      "\n",
      "Status: Confirmed\n",
      "\n",
      "Type: Animal\n",
      "\n",
      "Origin_year: 2013\n",
      "\n",
      "Origin_place: Tumblr\n",
      "\n",
      "Date_added: 2018-10-23T01:43:59-04:00\n",
      "\n",
      "Views: 12803434\n",
      "\n",
      "Videos: 62\n",
      "\n",
      "Photos: 1649\n",
      "\n",
      "Comments: 916\n",
      "\n",
      "Tags: animal, dog, shiba inu, shibe, such doge, super shibe, japanese, super, tumblr, much, very, many, comic sans, photoshop meme, such, shiba, shibe doge, doges, dogges, reddit, comic sans ms, tumblr meme\n",
      "\n",
      "About: Doge is a slang term for \"dog\" that is primarily associated with pictures of Shiba Inus (nicknamed \"Shibe\") and internal monologue captions on Tumblr. These photos may be photoshopped to change the do\n",
      "\n",
      "Origin: The use of the misspelled word \"doge\" to refer to a dog dates back to June 24th, 2005, when it was mentioned in an episode of Homestar Runner's puppet show. In the episode titled \"Biz Cas Fri 1\"[2], H\n",
      "\n",
      "Other_text: Identity On February 23rd, 2010, Japanese kindergarten teacher Atsuko Sato posted several photos of her rescue-adopted Shiba Inu dog Kabosu to her personal blog.[38] Among the photos included a peculi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_row = getMemeData('http://knowyourmeme.com/memes/doge')\n",
    "\n",
    "for key, value in data_row.items():\n",
    "    print(key.capitalize()+\":\", str(value)[:200], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бан снят. Стражники мемов ничего не заподозрили и пустили нас в сокровищницу. Чашу нашего респонса снова переполняет контент. Через силу мы обрели мощь. \n",
    "\n",
    "При желании, можно выяснить одну занимательную вещь: при базовых настройках, Тор-браузер меняет ip каждые 10 минут. Но что делать, если сервер банит нас быстрее?  Всё просто, в папке, куда был установлен Tor найдём файлик с настройками под названием torrc (на маке он лежит по адресу `~/Library/Application Support/TorBrowser-Data/torrc`, если не получится найти - добро пожаловать [сюда](https://tor.stackexchange.com/questions/11866/cant-find-torrc-file-on-mac)) и отредактируем его. Добавим строки: \n",
    "\n",
    "```\n",
    "CircuitBuildTimeout 10\n",
    "LearnCircuitBuildTimeout 0\n",
    "MaxCircuitDirtiness 10\n",
    "```\n",
    "\n",
    "Минимально возможный период для обновления ip составляет 10 секунд. Установим туда эту цифру и попробуем поиграться. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current IP Address: 89.31.57.5\n",
      "Current IP Address: 93.174.93.71\n",
      "Current IP Address: 62.210.207.52\n",
      "Current IP Address: 209.141.43.42\n",
      "Current IP Address: 209.141.43.42\n",
      "Current IP Address: 162.247.72.216\n",
      "Current IP Address: 185.220.101.17\n",
      "Current IP Address: 193.171.202.150\n",
      "Current IP Address: 128.31.0.13\n",
      "Current IP Address: 185.163.1.11\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    checkIP()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, смена ip происходит примерно раз в 10 секунд. Для наших целей по скачке мемов было достаточно и базовых настроек. Бан наступал примерно через 20 минут после начала работы кода. \n",
    "\n",
    "1. Открываем браузер;\n",
    "2. Запускаем кусок кода с подгрузкой библиотек;\n",
    "3. Запускаем цикл по мемам \n",
    "4. .....\n",
    "5. Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(columns=['name', 'status', 'type', 'origin_year', 'origin_place',\n",
    "                                 'date_added', 'views', 'videos', 'photos', 'comments', \n",
    "                                 'tags', 'about', 'origin', 'other_text'])\n",
    "\n",
    "\n",
    "for page_number in tqdm_notebook(range(1075), desc='Pages'):\n",
    "    # собрали хрефы с текущей страницы\n",
    "    meme_links = getPageLinks(page_number)  \n",
    "    for meme_link in tqdm_notebook(meme_links, desc='Memes', leave=False):\n",
    "        # иногда с первого раза страничка не парсится\n",
    "        for i in range(5):\n",
    "            try:\n",
    "                # пытаемся собрать по мему немного даты\n",
    "                data_row = getMemeData(meme_link)           \n",
    "                # и закидываем её в таблицу\n",
    "                final_df = final_df.append(data_row, ignore_index=True)  \n",
    "                # если всё получилось - выходим из внутреннего цикла\n",
    "                break\n",
    "            except:\n",
    "                # Иначе, пробуем еще несколько раз, пока не закончатся попытки\n",
    "                continue\n",
    "                \n",
    "final_df.to_csv('MEMES.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все мемы в наших руках. Можно приступать к варке фичей и моделированию. Через мощь мы познали победу. Остался только один вопрос: **Что, если мы хотим менять ip каждый реквест?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Путь второй \n",
    "\n",
    "Второй путь помогает извращаться со сменой ip как угодно. Зайдём на [Github каких-то ребят](https://github.com/alex-miller-0/Tor_Crawler) и скачаем себе их скрипт под названием `TorCrawler.py`. Все недостающие библиотеки, используемые в этом скрипте, придётся доставить. Поставим парням на репозиторий за их код звёздочку. Закинем этот скрипт либо в папку со своими библиотеками либо в папку к этому блокноту. Обратите внимание, что скрипт работает только с третьим питоном. \n",
    "\n",
    "Перед использованием библиотечки, нужно подкрутить настройки в torrc файлике. На маке он будет лежать в папке `/usr/local/etc/tor/`, на линуксе в папке `/etc/tor/`. Проследуем по инструкции авторов скрипта. \n",
    "\n",
    "1. Генерируем в консоли пароль `tor --hash-password mypassword` \n",
    "2. Открываем torrc файл в редакторе вроде [vim](https://stackoverflow.com/questions/11828270/how-to-exit-the-vim-editor), [nano](http://m.memegen.com/m4rnf5.jpg) или [atom](https://atom.io/) \n",
    "3. Сохраняем пароль в наш torrc-файл в строку, которая начинается с HashedControlPassword\n",
    "4. Раскоментируем строку, начинающуюся с HashedControlPassword\n",
    "5. Раскоментируем (если она закоментирована) строку ControlPort 9051\n",
    "6. Сохраним изменения. \n",
    "\n",
    "Запускаем tor в терминале. Линукс: `service tor start`, мак: `tor`.\n",
    "\n",
    "Теперь мы готовы парсить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from TorCrawler import TorCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём свой краулер, в опциях вводим пароль \n",
    "crawler = TorCrawler(ctrl_pass='mypassword')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем сделать get-запрос по аналогии с тем как делали раньше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_page = 'http://knowyourmeme.com/memes/doge'\n",
    "\n",
    "response = crawler.get(meme_page, headers={'User-Agent': UserAgent().chrome})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем ответ сразу же в формате bs4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим внутри что-нибудь нужное. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dd class=\"views\" title=\"12,318,185 Views\">\n",
       "<a href=\"/memes/doge\" rel=\"nofollow\">12,318,185</a>\n",
       "</dd>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = response.find('dd', attrs={'class':'views'})\n",
    "views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим IP адрес краулера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'51.15.40.23'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawler.ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По дефолту краулер меняет ip каждые $25$ запросов. За это отвечает параметр `n_requests`. При создании нового краулера, его можно настроить по собственному желанию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawler.n_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, ip можно при желании поменять вручную. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP successfully rotated. New IP: 62.176.4.1\n"
     ]
    }
   ],
   "source": [
    "crawler.rotate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь весь код для париснга можно перевести на рельсы этого небольшого скрипта, заменив все реквесты на торовские и менять IP адрес с каждым новым запросом к бедному серверу, на который внезапно обрушится огромное число запросов из всех уголков света.\n",
    "\n",
    "Победа порвала наши оковы. Великая Сила освободила нас.\n",
    "\n",
    "Напоследок, хотелось бы сказать пару слов о парсинге вообще и при помощи Тора в частности. Добывать себе данные - это стильно, модно и в принципе интересно, можно получить датасеты, которых еще никто никогда не обрабатывал, сделать что-то новое, посмотреть, наконец, на все мемы мира сразу. Однако не стоит забывать, что ограничения, введенные сервером, в том числе баны, появились не просто так, а в целях защиты сайта от недоброжелательных ковровых бомбардировок запросами и DDoS-атак. К чужому труду стоит относится с уважением, и даже если у сервера никакой защиты нет, - это еще не повод неограниченно забрасывать его реквестами, особенно если это может привести к его отключению - [уголовное наказание](http://sd-company.su/article/security/ddosataka-ugolovnaya-otvetstvennost) никто не отменял. Успешных и безопасных вам исследований!\n",
    "\n",
    "Авторы: Ульянкин Филипп @filfonul, Сергеев Дмитрий @Skolopendriy\n",
    "\n",
    "## Почиташки \n",
    "\n",
    "* [Годная книга](https://github.com/FUlyankin/Parsers/blob/master/Ryan_Mitchell_Web_Scraping_with_Python-_Collecting_Data_from_the_Modern_Web_2015.pdf) про парсинг на английском языке. \n",
    "* [Неплохая инструкция](https://jarroba.com/anonymous-scraping-by-tor-network/) о самостоятельном парсинге через Tor без использования чужих готовых классов. \n",
    "* [Димин репозиторий](https://github.com/DmitrySerg/memology) с исследованием мемов.\n",
    "* [Филин репозиторий](https://fulyankin.github.io/Parsers/) с ещё парой хорошо расписаных блокнотов с парсерами. Репозиторий изначально делался как страничка факультатива для студентов. \n",
    "* [Оригинальный кодекс](http://starwars.wikia.com/wiki/Code_of_the_Sith) адепта тёмной стороны силы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
